{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers datasets torch scikit-learn numpy -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:58:26.175354Z","iopub.execute_input":"2024-12-21T14:58:26.175701Z","iopub.status.idle":"2024-12-21T14:58:29.311685Z","shell.execute_reply.started":"2024-12-21T14:58:26.175674Z","shell.execute_reply":"2024-12-21T14:58:29.310528Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Importig Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    MBartForConditionalGeneration,\n    MBartTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nfrom datasets import load_dataset\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:58:32.771374Z","iopub.execute_input":"2024-12-21T14:58:32.771734Z","iopub.status.idle":"2024-12-21T14:58:38.870684Z","shell.execute_reply.started":"2024-12-21T14:58:32.771707Z","shell.execute_reply":"2024-12-21T14:58:38.869812Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Exploring Dataset Structure","metadata":{}},{"cell_type":"code","source":"# Load dataset and explore its structure\ndataset = load_dataset(\"SKNahin/bengali-transliteration-data\")\nprint(\"Dataset structure:\")\nprint(dataset)\n\n# Print column names\nprint(\"\\nColumns in training set:\")\nprint(dataset['train'].column_names)\n\n# Print a few examples\nprint(\"\\nFirst few examples:\")\nfor idx, example in enumerate(dataset['train'].select(range(3))):\n    print(f\"\\nExample {idx + 1}:\")\n    print(example)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:58:42.393745Z","iopub.execute_input":"2024-12-21T14:58:42.394355Z","iopub.status.idle":"2024-12-21T14:58:43.611423Z","shell.execute_reply.started":"2024-12-21T14:58:42.394324Z","shell.execute_reply":"2024-12-21T14:58:43.610546Z"}},"outputs":[{"name":"stdout","text":"Dataset structure:\nDatasetDict({\n    train: Dataset({\n        features: ['bn', 'rm'],\n        num_rows: 5006\n    })\n})\n\nColumns in training set:\n['bn', 'rm']\n\nFirst few examples:\n\nExample 1:\n{'bn': '‡¶∏‡ßç‡¶ï‡ßç‡¶∞‡ßã‡¶≤ ‡¶ï‡¶∞‡ßá ‡ß®‡ß¶/‡ß©‡ß¶ ‡¶∏‡ßá‡¶ï‡ßá‡¶®‡ßç‡¶° ‡¶è‡¶∞ ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶™‡¶æ‡¶® ‡¶®‡¶æ‡¶á???', 'rm': 'scroll kore 20/30 second er video pann nai???'}\n\nExample 2:\n{'bn': '‡¶ì ‡¶ó‡ßÅ‡¶≤‡¶æ ‡¶ü‡¶∞‡ßá‡¶®‡ßç‡¶ü ‡¶∏‡¶æ‡¶á‡¶ü ‡¶è ‡¶™‡¶æ‡¶¨‡ßá‡¶®', 'rm': 'o gula Torrent site e paben'}\n\nExample 3:\n{'bn': '‡¶≠‡¶ï‡ßç‡¶ï‡¶∞ ‡¶ö‡¶ï‡ßç‡¶ï‡¶∞ ‡¶™‡ßã‡¶∏‡ßç‡¶ü ‡¶è‡¶ï‡¶ü‡¶æ ‡¶ï‡¶∞‡¶≤‡ßá‡¶á ‡¶è‡¶™‡ßç‡¶∞‡ßÅ‡¶≠‡¶°.‚Ä¶ ‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶á  ‡¶ò‡¶æ‡¶¨‡¶≤‡¶æ ‡¶Ü‡¶õ‡ßá', 'rm': 'vokkor chokkor post akta korlei approved‚Ä¶. nishchoi ghabla ache'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Data loading Function","metadata":{}},{"cell_type":"code","source":"def load_and_preprocess_data():\n    # Load dataset from Hugging Face\n    dataset = load_dataset(\"SKNahin/bengali-transliteration-data\")\n    \n    # Extract Banglish (romanized) and Bengali texts\n    banglish_texts = dataset['train']['rm']  # 'rm' for romanized\n    bengali_texts = dataset['train']['bn']   # 'bn' for Bengali\n    \n    # Split into train and validation sets (90-10 split)\n    train_texts, val_texts, train_labels, val_labels = train_test_split(\n        banglish_texts, \n        bengali_texts,\n        test_size=0.1,\n        random_state=42\n    )\n    \n    return train_texts, val_texts, train_labels, val_labels\n\n# Load the data\ntrain_texts, val_texts, train_labels, val_labels = load_and_preprocess_data()\nprint(f\"Training samples: {len(train_texts)}\")\nprint(f\"Validation samples: {len(val_texts)}\")\n\n# Print a few examples\nprint(\"\\nFirst few examples:\")\nfor i in range(3):\n    print(f\"\\nExample {i+1}:\")\n    print(f\"Banglish: {train_texts[i]}\")\n    print(f\"Bengali: {train_labels[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:58:46.864709Z","iopub.execute_input":"2024-12-21T14:58:46.865044Z","iopub.status.idle":"2024-12-21T14:58:47.447873Z","shell.execute_reply.started":"2024-12-21T14:58:46.865014Z","shell.execute_reply":"2024-12-21T14:58:47.446905Z"}},"outputs":[{"name":"stdout","text":"Training samples: 4505\nValidation samples: 501\n\nFirst few examples:\n\nExample 1:\nBanglish: 2 minute ar account block kore dice‚Ä¶..post delete kore din\nBengali: ‡ß® ‡¶Æ‡¶ø‡¶®‡¶ø‡¶ü ‡¶è‡¶∞ ‡¶è‡¶ï‡¶æ‡¶â‡¶®‡ßç‡¶ü ‡¶¨‡ßç‡¶≤‡¶ï ‡¶ï‡¶∞‡ßá ‡¶¶‡¶ø‡¶õ‡ßá‚Ä¶..‡¶™‡ßã‡¶∏‡ßç‡¶ü ‡¶°‡¶ø‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡ßá ‡¶¶‡¶ø‡¶®\n\nExample 2:\nBanglish: Voy ke joy korun\nBengali: ‡¶≠‡ßü ‡¶ï‡ßá ‡¶ú‡ßü ‡¶ï‡¶∞‡ßÅ‡¶® \n\nExample 3:\nBanglish: apnar phoner net speed app er nam ta ki ar apps ta ki link dite parben ki\nBengali: ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶´‡ßã‡¶®‡ßá‡¶∞ ‡¶®‡ßá‡¶ü ‡¶∏‡ßç‡¶™‡¶ø‡¶° ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™ ‡¶è‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶ü‡¶æ ‡¶ï‡¶ø ‡¶Ü‡¶∞ ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶™‡ßç‡¶∏ ‡¶ü‡¶æ ‡¶ï‡¶ø ‡¶≤‡¶ø‡¶Ç‡¶ï ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá‡¶® ‡¶ï‡¶ø \n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Creating dataset class","metadata":{}},{"cell_type":"code","source":"class BanglishBengaliDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=128):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        # Encode Banglish input\n        text_encoding = self.tokenizer(\n            self.texts[idx],\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        \n        # Encode Bengali target\n        label_encoding = self.tokenizer(\n            self.labels[idx],\n            max_length=self.max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            'input_ids': text_encoding['input_ids'].squeeze(),\n            'attention_mask': text_encoding['attention_mask'].squeeze(),\n            'labels': label_encoding['input_ids'].squeeze()\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:58:50.837633Z","iopub.execute_input":"2024-12-21T14:58:50.837989Z","iopub.status.idle":"2024-12-21T14:58:50.844211Z","shell.execute_reply.started":"2024-12-21T14:58:50.837960Z","shell.execute_reply":"2024-12-21T14:58:50.843224Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Initializing Model and tokenizer","metadata":{}},{"cell_type":"code","source":"def prepare_model_and_tokenizer():\n    model_name = \"facebook/mbart-large-cc25\"\n    tokenizer = MBartTokenizer.from_pretrained(model_name)\n    model = MBartForConditionalGeneration.from_pretrained(model_name)\n    \n    # Set source and target language\n    tokenizer.src_lang = \"en_XX\"  # Using English tokens for Banglish\n    tokenizer.tgt_lang = \"bn_IN\"  # Bengali\n    \n    # Move model to GPU\n    model = model.to(device)\n    \n    return model, tokenizer\n\n# Create model and tokenizer\nmodel, tokenizer = prepare_model_and_tokenizer()\nprint(\"Model loaded and moved to GPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:58:54.503174Z","iopub.execute_input":"2024-12-21T14:58:54.503517Z","iopub.status.idle":"2024-12-21T14:58:59.837350Z","shell.execute_reply.started":"2024-12-21T14:58:54.503488Z","shell.execute_reply":"2024-12-21T14:58:59.836567Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Model loaded and moved to GPU\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Creating evaluation Matrix","metadata":{}},{"cell_type":"code","source":"def levenshtein_distance(s1, s2):\n    if len(s1) < len(s2):\n        return levenshtein_distance(s2, s1)\n\n    if len(s2) == 0:\n        return len(s1)\n\n    previous_row = range(len(s2) + 1)\n    for i, c1 in enumerate(s1):\n        current_row = [i + 1]\n        for j, c2 in enumerate(s2):\n            insertions = previous_row[j + 1] + 1\n            deletions = current_row[j] + 1\n            substitutions = previous_row[j] + (c1 != c2)\n            current_row.append(min(insertions, deletions, substitutions))\n        previous_row = current_row\n\n    return previous_row[-1]\n\n# Enhanced compute_metrics function with more metrics\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Calculate character error rate (CER)\n    total_cer = 0\n    total_matches = 0\n    total_chars = 0\n    \n    for pred, label in zip(decoded_preds, decoded_labels):\n        distance = levenshtein_distance(pred, label)\n        total_cer += distance / len(label)\n        \n        # Calculate exact matches\n        if pred == label:\n            total_matches += 1\n            \n        # Calculate character-level accuracy\n        total_chars += len(label)\n    \n    # Calculate metrics\n    cer = total_cer / len(decoded_labels)\n    exact_match_ratio = total_matches / len(decoded_labels)\n    \n    return {\n        \"character_error_rate\": cer,\n        \"exact_match_ratio\": exact_match_ratio,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:59:04.732456Z","iopub.execute_input":"2024-12-21T14:59:04.732931Z","iopub.status.idle":"2024-12-21T14:59:04.739862Z","shell.execute_reply.started":"2024-12-21T14:59:04.732893Z","shell.execute_reply":"2024-12-21T14:59:04.738964Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Creating training Dataset","metadata":{}},{"cell_type":"code","source":"# Create training and validation datasets\ntrain_dataset = BanglishBengaliDataset(train_texts, train_labels, tokenizer)\nval_dataset = BanglishBengaliDataset(val_texts, val_labels, tokenizer)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:59:08.606708Z","iopub.execute_input":"2024-12-21T14:59:08.607021Z","iopub.status.idle":"2024-12-21T14:59:08.612137Z","shell.execute_reply.started":"2024-12-21T14:59:08.606998Z","shell.execute_reply":"2024-12-21T14:59:08.611298Z"}},"outputs":[{"name":"stdout","text":"Training dataset size: 4505\nValidation dataset size: 501\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Setting up Training arguments and Trainner","metadata":{}},{"cell_type":"code","source":"# Training arguments with matching save and eval strategies\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./banglish-bengali-translator\",\n    evaluation_strategy=\"steps\",     # Evaluate every N steps\n    save_strategy=\"steps\",          # Changed to match evaluation_strategy\n    eval_steps=100,                 # Evaluate every 100 steps\n    save_steps=100,                 # Save every 100 steps (matching eval_steps)\n    logging_strategy=\"steps\",       # Log metrics\n    logging_steps=5,               # Log every 5 steps\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,   # Adjust if needed based on GPU memory\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=3,\n    predict_with_generate=True,\n    fp16=True,                       # Enable mixed precision training\n    no_cuda=False,                   # Enable GPU usage\n    load_best_model_at_end=True,    # Load the best model when training ends\n    metric_for_best_model=\"character_error_rate\",  # Use CER to determine best model\n    greater_is_better=False         # Lower CER is better\n)\n\n# Initialize trainer with progress bar\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:59:11.352974Z","iopub.execute_input":"2024-12-21T14:59:11.353268Z","iopub.status.idle":"2024-12-21T14:59:11.822521Z","shell.execute_reply.started":"2024-12-21T14:59:11.353247Z","shell.execute_reply":"2024-12-21T14:59:11.821876Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Traning the model","metadata":{}},{"cell_type":"code","source":"print(\"\\nStarting training...\")\ntrain_result = trainer.train()\n\n# Print training metrics\nprint(\"\\nTraining completed. Final metrics:\")\nprint(f\"Training loss: {train_result.training_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-21T14:59:17.376063Z","iopub.execute_input":"2024-12-21T14:59:17.376374Z"}},"outputs":[{"name":"stdout","text":"\nStarting training...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Simple Eval","metadata":{}},{"cell_type":"code","source":"# Evaluate on the validation set\neval_results = trainer.evaluate()\nprint(\"\\nFinal Evaluation Results:\")\nfor metric, value in eval_results.items():\n    print(f\"{metric}: {value:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the model","metadata":{}},{"cell_type":"code","source":"# Save the model and tokenizer\noutput_dir = \"./banglish-bengali-translator-final\"\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nprint(f\"Model saved to {output_dir}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing the Model with Some Examples","metadata":{}},{"cell_type":"code","source":"def translate_text(text, model, tokenizer):\n    # Prepare the text into tokenized ids\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n    \n    # Move to GPU if available\n    if torch.cuda.is_available():\n        inputs = {k: v.to('cuda') for k, v in inputs.items()}\n        model = model.to('cuda')\n    \n    # Generate translation\n    translated = model.generate(\n        **inputs,\n        max_length=128,\n        num_beams=4,\n        length_penalty=1.0,\n        early_stopping=True,\n        forced_bos_token_id=tokenizer.lang_code_to_id[\"bn_IN\"]\n    )\n    \n    # Decode the generated tokens to text\n    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n    return translated_text\n\n# Test with some examples\ntest_examples = [\n    \"ami tomake bhalobashi\",\n    \"tumi kemon acho\",\n    \"bangla bhasha amader praner bhasha\"\n]\n\nprint(\"\\nTesting the model with some examples:\")\nfor text in test_examples:\n    translated = translate_text(text, model, tokenizer)\n    print(f\"\\nInput (Banglish): {text}\")\n    print(f\"Output (Bengali): {translated}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}